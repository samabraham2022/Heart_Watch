{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the libraries\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from scipy.signal import medfilt, butter, filtfilt\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.signal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Reshape\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import shape\n",
    "\n",
    "\n",
    "def featureselection(X_train):\n",
    "    X_train=np.array(X_train)\n",
    "    features = []\n",
    "    print(X_train.shape)\n",
    "    # Extracting features for each sample\n",
    "    for i in range(1):\n",
    "        #Finding the R-peaks\n",
    "        print(X_train[i])\n",
    "        r_peaks = scipy.signal.find_peaks(X_train[i])[0]\n",
    "        #Initialize lists to hold R-peak and T-peak amplitudes\n",
    "        r_amplitudes = []\n",
    "        t_amplitudes = []\n",
    "        # Iterate through R-peak locations to find corresponding T-peak amplitudes\n",
    "        for r_peak in r_peaks:\n",
    "        # Find the index of the T-peak (minimum value) in the interval from R-peak to R-peak + 200 samples\n",
    "            t_peak = np.argmin(X_train[i][r_peak:r_peak+200]) + r_peak\n",
    "            #Append the R-peak amplitude and T-peak amplitude to the lists\n",
    "            r_amplitudes.append(X_train[i][r_peak])\n",
    "            t_amplitudes.append(X_train[i][t_peak])\n",
    "        # extracting singular value metrics from the r_amplitudes\n",
    "        std_r_amp = np.std(r_amplitudes)\n",
    "        mean_r_amp = np.mean(r_amplitudes)\n",
    "        median_r_amp = np.median(r_amplitudes)\n",
    "        sum_r_amp = np.sum(r_amplitudes)\n",
    "        # extracting singular value metrics from the t_amplitudes\n",
    "        std_t_amp = np.std(t_amplitudes)\n",
    "        mean_t_amp = np.mean(t_amplitudes)\n",
    "        median_t_amp = np.median(t_amplitudes)\n",
    "        sum_t_amp = np.sum(t_amplitudes)\n",
    "        # Find the time between consecutive R-peaks\n",
    "        rr_intervals = np.diff(r_peaks)\n",
    "        # Calculate the time duration of the data collection\n",
    "        time_duration = (len(X_train[i]) - 1) / 1000 # assuming data is in ms\n",
    "        # Calculate the sampling rate\n",
    "        sampling_rate = len(X_train[i]) / time_duration\n",
    "        # Calculate heart rate\n",
    "        duration = len(X_train[i]) / sampling_rate\n",
    "        heart_rate = (len(r_peaks) / duration) * 60\n",
    "        # QRS duration\n",
    "        qrs_duration = []\n",
    "        for j in range(len(r_peaks)):\n",
    "            qrs_duration.append(r_peaks[j]-r_peaks[j-1])\n",
    "        # extracting singular value metrics from the qrs_durations\n",
    "        std_qrs = np.std(qrs_duration)\n",
    "        mean_qrs = np.mean(qrs_duration)\n",
    "        median_qrs = np.median(qrs_duration)\n",
    "        sum_qrs = np.sum(qrs_duration)\n",
    "        # Extracting the singular value metrics from the RR-interval\n",
    "        std_rr = np.std(rr_intervals)\n",
    "        mean_rr = np.mean(rr_intervals)\n",
    "        median_rr = np.median(rr_intervals)\n",
    "        sum_rr = np.sum(rr_intervals)\n",
    "        # Extracting the overall standard deviation\n",
    "        std = np.std(X_train[i])\n",
    "        # Extracting the overall mean\n",
    "        mean = np.mean(X_train[i])\n",
    "        # Appending the features to the list\n",
    "        features.append([mean, std, std_qrs, mean_qrs,median_qrs, sum_qrs, std_r_amp, mean_r_amp, median_r_amp, sum_r_amp, std_t_amp, mean_t_amp, median_t_amp, sum_t_amp, sum_rr, std_rr, mean_rr,median_rr, heart_rate])\n",
    "    # Converting the list to a numpy array\n",
    "    features = np.array(features)\n",
    "    num_features = features.shape[1]\n",
    "    # Reshape the features data to be in the right shape for LSTM input\n",
    "    features = features.reshape(features.shape[0], features.shape[1], 1)\n",
    "    print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(ecg_data):\n",
    "    ecg_data=pd.DataFrame(ecg_data)\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    ecg_data = scaler.fit_transform(ecg_data)\n",
    "    from scipy.signal import medfilt, butter, filtfilt\n",
    "    import pywt\n",
    "    import numpy as np\n",
    "    #filtering the raw signals\n",
    "    # Median filtering\n",
    "    ecg_medfilt = medfilt(ecg_data, kernel_size=3)\n",
    "    # Low-pass filtering\n",
    "    lowcut = 0.05\n",
    "    highcut = 20.0\n",
    "    nyquist = 0.5 * 360.0\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(4, [low, high], btype='band')\n",
    "    ecg_lowpass = filtfilt(b, a, ecg_data)\n",
    "    # Wavelet filtering\n",
    "    coeffs = pywt.wavedec(ecg_data, 'db4', level=1)\n",
    "    threshold = np.std(coeffs[-1]) * np.sqrt(2*np.log(len(ecg_data)))\n",
    "    coeffs[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeffs[1:])\n",
    "    ecg_wavelet = pywt.waverec(coeffs, 'db4')\n",
    "    return ecg_wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the final function that you have to use for prediction. But the above 2 functions are needed to be called within this function. The input to this function is a list of ECG values similar to 1 row of the dataset.(140 values). Normalization and feature selection would be done automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 140)\n",
      "[-0.48104583 -0.89507291 -1.6139316  -2.3150379  -2.8132684  -3.4483725\n",
      " -3.7958751  -3.724429   -3.233412   -2.5728547  -1.7754831  -1.3752221\n",
      " -1.2717754  -1.1769392  -0.69896056 -0.32767706 -0.34774622 -0.25987326\n",
      " -0.29708395 -0.26650075 -0.19947271 -0.23269771 -0.12767905 -0.1117111\n",
      " -0.21274644 -0.23851323 -0.29606666 -0.18276302 -0.23140758 -0.27732169\n",
      " -0.21509662 -0.15659865 -0.29736812 -0.34161522 -0.20570406 -0.17081856\n",
      " -0.23600654 -0.25926463 -0.23550374 -0.20075316 -0.09247985 -0.10157705\n",
      " -0.21069194 -0.23446761 -0.26356206 -0.23040141 -0.25739743 -0.33296364\n",
      " -0.30888389 -0.25655495 -0.20475829 -0.26725194 -0.29689288 -0.2736634\n",
      " -0.27730672 -0.2756215  -0.27679635 -0.34274949 -0.34614008 -0.32344645\n",
      " -0.29953083 -0.33101675 -0.34223774 -0.24952694 -0.31030925 -0.27540634\n",
      " -0.26717798 -0.31483612 -0.26695004 -0.21320208 -0.14146621 -0.28237629\n",
      " -0.11932984  0.06128687 -0.0161068   0.02818601 -0.00589579  0.05898221\n",
      "  0.0302873   0.05980457  0.13481763  0.05191374  0.02352851  0.11131851\n",
      "  0.10900822  0.21506647  0.35952225  0.29389489  0.34949523  0.34691705\n",
      "  0.34566461  0.38798981  0.35027878  0.31754845  0.32532177  0.37316969\n",
      "  0.44590344  0.44310561  0.4620705   0.37547482  0.4626118   0.58898671\n",
      "  0.69865112  0.77507944  0.66532271  0.7015413   0.69210957  0.86239027\n",
      "  0.81728807  0.77195091  0.84156005  0.95431529  1.0621725   1.0188628\n",
      "  1.0345506   1.0649126   1.0727023   1.1687993   1.1585265   1.1847479\n",
      "  1.1927132   1.2242827   1.5376262   1.7457357   2.0887828   2.2387501\n",
      "  2.0261496   1.6500892   1.2427477   0.81901064  0.42554243  0.68517545\n",
      "  1.4202206   1.6920042   1.0777338   0.5638955   0.20478047  0.45765847\n",
      " -0.01203899 -0.2098525 ]\n",
      "[[[ 2.78571325e-11]\n",
      "  [ 9.96422174e-01]\n",
      "  [ 2.12920864e+01]\n",
      "  [ 0.00000000e+00]\n",
      "  [ 3.00000000e+00]\n",
      "  [ 0.00000000e+00]\n",
      "  [ 5.99948930e-01]\n",
      "  [ 2.32777273e-01]\n",
      "  [ 4.35841125e-02]\n",
      "  [ 7.91442727e+00]\n",
      "  [ 6.51524247e-02]\n",
      "  [-2.70203481e-01]\n",
      "  [-2.09852500e-01]\n",
      "  [-9.18691837e+00]\n",
      "  [ 1.22000000e+02]\n",
      "  [ 1.54693927e+00]\n",
      "  [ 3.69696970e+00]\n",
      "  [ 3.00000000e+00]\n",
      "  [ 1.46762590e+04]]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samab\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\signal\\_signaltools.py:1551: UserWarning: kernel_size exceeds volume extent: the volume will be zero-padded.\n",
      "  warnings.warn('kernel_size exceeds volume extent: the volume will be '\n"
     ]
    }
   ],
   "source": [
    "def model_predict(l):\n",
    "    test=format_data(l)\n",
    "    test=featureselection(l)\n",
    "    pred=model.predict(test)\n",
    "    if pred>=0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "a = []\n",
    "out = np.array(a)\n",
    "print(model_predict([out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
